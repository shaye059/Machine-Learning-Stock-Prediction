{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+vcFSwtStacYOnxDJKWmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaye059/Machine-Learning-Stock-Prediction/blob/master/Stock_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo-KlljvWLyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "55b6d82e-d64b-4478-a2d8-13711b2d8517"
      },
      "source": [
        "!pip install newsapi-python\n",
        "!pip install -q wordcloud\n",
        "!pip install yfinance\n",
        "\n",
        "import wordcloud\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import yfinance as yf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading https://files.pythonhosted.org/packages/de/9e/9050199ac7cbc755d1c49577fdaa5517901124b574264b3602a8b8028440/newsapi_python-0.2.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from newsapi-python) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->newsapi-python) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->newsapi-python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->newsapi-python) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->newsapi-python) (1.24.3)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.6\n",
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/31/8b374a12b90def92a4e27d0fc595fc43635f395984e36a075244d98bd265/yfinance-0.1.54.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.6.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.54-py2.py3-none-any.whl size=22409 sha256=5b8336f4a7c349803119594826b6557e4cffbd6b40573dee805afe820ece4db8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/e3/5b/ec24dd2984b12d61e0abf26289746c2436a0e7844f26f2515c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: yfinance\n",
            "Successfully installed yfinance-0.1.54\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHxFJC7NRz94",
        "colab_type": "text"
      },
      "source": [
        "##Step 1 - Data extraction\n",
        "Call the Google News API to get a list of articles for each company in a list. Since the model will be prediciting the effects of news stories, the companies should be as large as possible to ensure a greater variation in who holds stock, which in turn will mean a longer time for the news to spread to most shareholders. I'll also be limiting the companies to the tech sector to make it easier for the model to find trends in articles.\n",
        "\n",
        "I've written the articles to a CSV file so that this portion does not need to be rerun unless you want to modify the companies or get an updated list of articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj85PVbefxds",
        "colab_type": "text"
      },
      "source": [
        "#### Don't run this section unless you modify the list of companies or want updated articles\n",
        "I've saved the values into a CSV file that they can be read from.\n",
        "\n",
        "If you do want to run this section you need a News API key.\n",
        "\n",
        "-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gy5ubQVKy26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "newsapi = NewsApiClient(api_key='') #Your Key Here\n",
        "\n",
        "list_of_articles = []\n",
        "dict_of_companies = {'Microsoft': 'MSFT','Apple':'AAPL','Amazon':'AMZN',\n",
        "                     'Alphabet':'GOOGL','Facebook':'FB','Intel':'INTC',\n",
        "                     'Cisco Systems':'CSCO','Comcast':'CMCSA','Adobe':'ADBE',\n",
        "                     'Nvidia':'NVDA','Netflix':'NFLX','PayPal':'PYPL',\n",
        "                     'Broadcom':'AVGO','Alibaba':'BABA','AT&T':'T',\n",
        "                     'Tencent':'TCEHY','Taiwan Semiconductor':'TSM',\n",
        "                     'Verizon':'VZ', 'Oracle':'ORCL','Tesla':'TSLA',\n",
        "                     'Qualcomm':'QCOM','Texas Instruments':'TXN','Fiserv':'FISV',\n",
        "                     'Booking Holdings':'BKNG','Intuit':'INTU','ADP':'ADP',\n",
        "                     'T-Mobile':'TMUS','Micron':'MU','SAP':'SAP',\n",
        "                     'Salesforce':'CRM','IBM':'IBM','VMware':'VMW',\n",
        "                     'Samsung':'005930.KS','Foxconn':'2354.TW','Dell':'DELL',\n",
        "                     'Sony':'SNE','Panasonic':'PCRFY','HP':'HP'}\n",
        "\n",
        "# call the news api to retrieve a list of articles about Tesla\n",
        "for company,ticker in dict_of_companies.items():\n",
        "  temp = newsapi.get_everything(\n",
        "                                        language='en', \n",
        "                                        q=company)['articles']\n",
        "  for article in temp:\n",
        "    article['company'] = company\n",
        "    article['ticker'] = ticker\n",
        "  list_of_articles = list_of_articles + temp\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI1ZNdT1e1UP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "df3f480f-9571-4a83-e6d6-ac44832ec129"
      },
      "source": [
        "print(len(list_of_articles))\n",
        "print(list_of_articles[0])\n",
        "print(list_of_articles[-1])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "760\n",
            "{'source': {'id': 'wired', 'name': 'Wired'}, 'author': 'Lauren Goode', 'title': 'Microsoft Duo: Price, Details, Release Date', 'description': 'Microsoft has had to shift its sales pitch from a “work” product to “staying home indefinitely” device.', 'url': 'https://www.wired.com/story/microsoft-surface-duo-price-release-date/', 'urlToImage': 'https://media.wired.com/photos/5f32e4cd37e06d513ea90690/191:100/w_1280,c_limit/Gear-Duo-Hero_01.jpg', 'publishedAt': '2020-08-12T13:00:00Z', 'content': 'The context of these kinds of devices has changed, says Ben Arnold, consumer technology analyst at the NPD Group, which tracks US sales of electronics. Its not the one-handed emailer on the subway an… [+2086 chars]', 'company': 'Microsoft', 'ticker': 'MSFT'}\n",
            "{'source': {'id': 'engadget', 'name': 'Engadget'}, 'author': 'Valentina Palladino', 'title': 'The best affordable Windows laptops you can buy', 'description': 'If you’re a regular Engadget reader, you probably don’t think of cheap Windows laptops when you think of daily drivers. But it would be a big mistake to ignore these devices -- if not for yourself, for others you may know. There’s a reason why companies like …', 'url': 'https://www.engadget.com/best-affordable-windows-laptops-123000512.html', 'urlToImage': 'https://o.aolcdn.com/images/dims?resize=1200%2C630&crop=1200%2C630%2C0%2C0&quality=95&image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2020-08%2F35e6f910-d662-11ea-9df6-771467197b3d&client=amp-blogside-v2&signature=019e41873ad2354848142b6bc217ce0b2895fdb1', 'publishedAt': '2020-08-07T12:30:00Z', 'content': 'The Chromebook question\\r\\nNow, you may be inclined to recommend a Chromebook or a tablet to all of the people listed above. Those instincts aren’t wrong, but Chromebooks and tablets aren’t for everyon… [+10378 chars]', 'company': 'HP', 'ticker': 'HP'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvpuYJQJSWUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(list_of_articles)\n",
        "df.to_csv('News_Articles.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moeH0V9lhOGk",
        "colab_type": "text"
      },
      "source": [
        "-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opLAuNgYSMJt",
        "colab_type": "text"
      },
      "source": [
        "### Reading from CSV\n",
        "Read articles from CSV file into a Pandas Dataframe. I'm going to keep the title, description, and the date/time published so in the second line below I drop all other values for each article.\n",
        "\n",
        "**NOTE: Before running this section you need to add the 'News_Articles.csv' file to the colab workspace. You can find the file on the project Github:** https://github.com/shaye059/Machine-Learning-Stock-Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_jZb3ADfl85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0766d6f3-78a3-4484-90b9-6fe44c48a38a"
      },
      "source": [
        "articles = pd.read_csv('News_Articles.csv')\n",
        "\n",
        "articles.drop(articles.columns.difference(['title','description','publishedAt','company','ticker']), 1, inplace=True)\n",
        "\n",
        "\n",
        "print(articles.iloc[[0, -1]])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               title  ... ticker\n",
            "0        Microsoft Duo: Price, Details, Release Date  ...   MSFT\n",
            "760  The best affordable Windows laptops you can buy  ...     HP\n",
            "\n",
            "[2 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIrnhKeKrnTN",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 - Preprocessing\n",
        "Some simple POS lemmatization of the titles and descriptions to make it easier for the model to find trends.\n",
        "\n",
        "*Note for later: There may be room to improve this by removing the companies name from the article. This may help if the model appears to be overfitting so I'll come back and change this based on performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIiXC7doqTc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: Lemmatize and POS\n",
        "\n",
        "\n",
        "# helper function to tag words with their wordnet tag based on the nltk POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.ADV"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3kCztV0TDkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "def pos_lemmatize(string):\n",
        "  # Cast to string to ensure all of them are\n",
        "  string = str(string)\n",
        "  \n",
        "  # Tokenize\n",
        "  tokens = nltk.word_tokenize(string.lower())\n",
        "\n",
        "  # Keep only alpha numerics\n",
        "  tokens_alpha = [t for t in tokens if re.match(\"^[a-zA-Z]+$\", t)]\n",
        "\n",
        "  # Remove stopwords\n",
        "  tokens_alpha_stopless = [w for w in tokens_alpha if not w in stopwords.words('english')]\n",
        "\n",
        "  # POS Tag\n",
        "  pos_tokens = nltk.pos_tag(tokens_alpha_stopless)\n",
        "\n",
        "  # POS-based lemmatization\n",
        "  wordnet_tags = [get_wordnet_pos(p[1]) for p in pos_tokens]\n",
        "  title_pos_lemmas = [wnl.lemmatize(t, w) for t, w in zip(tokens_alpha_stopless, wordnet_tags)]\n",
        "\n",
        "  new_text = \" \".join(title_pos_lemmas)\n",
        "\n",
        "  return new_text\n",
        "\n",
        "articles['title'] = articles['title'].apply(pos_lemmatize)\n",
        "articles['description'] = articles['description'].apply(pos_lemmatize)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1q_VnJL4maK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "30a9cbed-adc3-4d05-9683-0821867c97f8"
      },
      "source": [
        "#pd.options.display.max_colwidth = 100\n",
        "\n",
        "print(articles['title'].iloc[0:3])\n",
        "print(articles['description'].iloc[0:3])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0              microsoft duo price detail release date\n",
            "1    microsoft signal renew interest windows late r...\n",
            "2    microsoft launch new family safety app io android\n",
            "Name: title, dtype: object\n",
            "0    microsoft shift sale pitch work product stay h...\n",
            "1    microsoft reorganizing window team windows dev...\n",
            "2    microsoft launch new family safety app io andr...\n",
            "Name: description, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMRdOQvpumD7",
        "colab_type": "text"
      },
      "source": [
        "Now get stock information from when each article was published to see its affect on the price. Start by comparing the opening price on the day the article was published to the closing price. If the article was published before/after trading hours, or on a non-trading day, then I'll look at the previous close and/or next open."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Kdy6f39jrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "temp_list = articles.to_dict('records')\n",
        "temp_list2 = list(filter(lambda i: type(i['publishedAt']) != float, temp_list))\n",
        "article_data = list(filter(lambda i: type(i['ticker']) != float, temp_list2))\n",
        "\n",
        "for article in article_data:\n",
        "  if not (type(article['publishedAt']) == float ):\n",
        "    published = article['publishedAt']\n",
        "    published_date = published[0:10]\n",
        "    published_date = date(int(published_date[0:4]),int(published_date[5:7]),int(published_date[8:10]))\n",
        "    \n",
        "    # Check if the article was published on a trading day\n",
        "    is_market_day = bool(len(pd.bdate_range(published_date, published_date)))\n",
        "    \n",
        "    # If it was a market day, see if it was published during trading hours\n",
        "    if is_market_day:\n",
        "      time = published[11:-1]\n",
        "\n",
        "      # If the article was published before trading hours, compare the price from\n",
        "      # the close of the previous day to the open of the current day.\n",
        "      if time < '09:30:00':\n",
        "        prev_day = published_date - timedelta(1)\n",
        "        prev_market_day = bool(len(pd.bdate_range(prev_day, prev_day)))\n",
        "        while not prev_market_day:\n",
        "          prev_day = prev_day - timedelta(1)\n",
        "          prev_market_day = bool(len(pd.bdate_range(prev_day, prev_day)))\n",
        "        \n",
        "        # A time delta of one day is added to each end day since yFinance only \n",
        "        # retrieves data upto and NOT including the end date.\n",
        "        end = published_date + timedelta(1)\n",
        "\n",
        "        article['start'], article['end'] = (prev_day, end)\n",
        "\n",
        "      # The market typically closes at 16:00 (4pm) but the later limit\n",
        "      # will be set to 15:30 (3:30pm) since most people will not have seen it\n",
        "      # within the 30 minutes before closing. Compare close of current day to open\n",
        "      # of next market day.\n",
        "      elif time > '15:30:00':\n",
        "        next_day = published_date + timedelta(1)\n",
        "        next_market_day = bool(len(pd.bdate_range(next_day, next_day)))\n",
        "        while not next_market_day:\n",
        "          next_day = next_day + timedelta(1)\n",
        "          next_market_day = bool(len(pd.bdate_range(next_day, next_day)))\n",
        "        end = next_day + timedelta(1)\n",
        "        article['start'], article['end'] = published_date, end\n",
        "\n",
        "      # The article was published during market hours so compare opening of\n",
        "      # current day to close of current day\n",
        "      else:\n",
        "        article['start'] = published_date\n",
        "        article['end'] = published_date + timedelta(1)\n",
        "    \n",
        "    # The article was posted on a day that the market was closed so compare the\n",
        "    # close of the last market day to the opening of the next.\n",
        "    else:\n",
        "      prev_day = published_date - timedelta(1)\n",
        "      prev_market_day = bool(len(pd.bdate_range(prev_day, prev_day)))\n",
        "      next_day = published_date + timedelta(1)\n",
        "      next_market_day = bool(len(pd.bdate_range(next_day, next_day)))\n",
        "      while not prev_market_day:\n",
        "        prev_day = prev_day - timedelta(1)\n",
        "        prev_market_day = bool(len(pd.bdate_range(prev_day, prev_day)))\n",
        "      while not next_market_day:\n",
        "          next_day = next_day + timedelta(1)\n",
        "          next_market_day = bool(len(pd.bdate_range(next_day, next_day)))\n",
        "      article['start'] = prev_day\n",
        "      article['end'] = next_day + timedelta(1)\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy7Xg5Lad3Kt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "204fe6a1-e892-453d-aa3f-b85d1179e7b1"
      },
      "source": [
        "for article in article_data[0:3]:\n",
        "  print('Start: ' + str(article['start']) + ' End: ' + str(article['end']))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start: 2020-08-12 End: 2020-08-13\n",
            "Start: 2020-08-07 End: 2020-08-08\n",
            "Start: 2020-07-28 End: 2020-07-30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erFH5-ZTCTvt",
        "colab_type": "text"
      },
      "source": [
        "#### Ticker Data Retrieval\n",
        "This first cell retrieves the data from yFinance. If an error occurs while running the second cell below, try rerunning the first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeigLqcE8_z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ticker symbol for the company we want\n",
        "tickers = []\n",
        "\n",
        "\n",
        "# This could iterate through dict_of_companies, will seperate that out and chage \n",
        "# this part later\n",
        "for article in article_data:\n",
        "  if not article['ticker'] in tickers:\n",
        "    tickers.append(article['ticker'])\n",
        "\n",
        "tickerData = {}\n",
        "for ticker in tickers:\n",
        "\n",
        "  tickerData[ticker] = yf.Ticker(ticker)\n",
        "#tickerSymbol = 'AMZN'\n",
        "\n",
        "#tickerData = yf.Ticker(tickerSymbol)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPyshN__imJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: Save the delta for each article. Possibly remove start, end, and\n",
        "##       publishedAt from each one so that there's only the necessary data.\n",
        "for article in article_data:\n",
        "\n",
        "  try:\n",
        "    tickeramzn = tickerData[article['ticker']].history(interval='1d', start=str(article['start']), end=str(article['end']))\n",
        "    if len(tickeramzn) == 1:\n",
        "      day = tickeramzn.iloc[0]\n",
        "      delta = (day['Close'] - day['Open']) / day['Open'] *100\n",
        "      article['Delta'] = delta\n",
        "\n",
        "    if len(tickeramzn) == 2:\n",
        "      day1 = tickeramzn.iloc[0]\n",
        "      day2 = tickeramzn.iloc[1]\n",
        "      delta = (day2['Open'] - day1['Close']) / day1['Close'] *100\n",
        "      article['Delta'] = delta\n",
        "  except:\n",
        "    print('Error retrieving values from yFinance, try running first cell again')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvjeJ-TNBiYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "326654f8-40b5-45ef-ed36-360d15514089"
      },
      "source": [
        "for article in article_data[0:3]:\n",
        "  print(article['Delta'])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8997515709484172\n",
            "-1.1030951826855968\n",
            "0.23760023760023252\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}